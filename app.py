import streamlit as st
import google.generativeai as genai
from PIL import Image
import io
import os

# --- Configuraci√≥n de la P√°gina ---
st.set_page_config(
    page_title="AliadoDoc",
    page_icon="üß†",
    layout="wide"
)

# --- SISTEMA DE CONDUCTA (TU GEM) ---
# üö® PEGA TU PROMPT COMPLETO DE CONDUCTA AQU√ç üö®
SISTEMA_DE_CONDUCTA = os.environ.get("GEM_PROMPT", 'aaa')

# --- Barra Lateral para Configuraci√≥n ---
with st.sidebar:
    
    st.title("AliadoDoc")
    st.header("‚öôÔ∏è Configuraci√≥n")
    
    # 1. Cargar API Key desde Colab Secrets
    api_key = os.environ.get("GEMINI_API_KEY", 'aaa')     
    
    
    model_option = st.selectbox(
        "Selecciona el Modelo",
        ("gemini-2.5-pro", "gemini-2.5-flash")
    )
 
# --- Funciones Auxiliares ---
def get_gemini_response(api_key, model_name, user_prompt, system_instruction, content_files=None):
    """Funci√≥n para interactuar con la API de Gemini."""
    # En un entorno real (no Colab con variables de entorno), esto fallar√≠a,
    if not api_key:
        return "‚ö†Ô∏è Por favor, aseg√∫rate de que la variable GEMINI_API_KEY est√© configurada."
    
    try:
        genai.configure(api_key=api_key)
        
        # Inicializar el modelo con la instrucci√≥n de sistema
        model = genai.GenerativeModel(
            model_name=model_name,
            system_instruction=system_instruction
        )
        
        generation_parts = [user_prompt]
        if content_files:
            for file in content_files:
                generation_parts.append(file)
        
        # Generar respuesta (stream=True para efecto de escritura)
        response = model.generate_content(generation_parts, stream=True)
        return response
    except Exception as e:
        return f"‚ùå Error: {str(e)}"

def process_uploaded_file(uploaded_file):
    """Procesa el archivo subido y lo convierte al formato que Gemini entiende."""
    if uploaded_file is None: return None
    mime_type = uploaded_file.type
    
    # Si es imagen
    if mime_type.startswith('image'):
        try:
            # Usamos read() y BytesIO para que PIL pueda abrir el archivo sin guardarlo en disco
            return Image.open(io.BytesIO(uploaded_file.read()))
        except: 
            st.error("Error al procesar la imagen. Aseg√∫rese de que el formato es v√°lido.")
            return None
            
    # Si es texto (txt, py, md, csv, etc.)
    elif mime_type.startswith('text') or mime_type == 'application/json':
        try:
            # Volvemos al inicio del buffer antes de leer
            uploaded_file.seek(0)
            return uploaded_file.read().decode("utf-8")
        except: 
            st.error("Error al leer el archivo de texto.")
            return None
            
    return None

# --- Interfaz Principal ---

# Inicializar sesi√≥n para gesti√≥n de archivos
if 'uploaded_file_data' not in st.session_state:
    st.session_state.uploaded_file_data = None
if 'uploaded_file_name' not in st.session_state:
    st.session_state.uploaded_file_name = None
# Inicializar variable para prompt generado por bot√≥n
if 'prompt_from_button' not in st.session_state:
    st.session_state.prompt_from_button = None


# --- Obtener contenido de la sesi√≥n para usar y mostrar ---
processed_content = st.session_state.uploaded_file_data
file_name_to_display = st.session_state.uploaded_file_name

# Mostrar la previsualizaci√≥n del contenido guardado en la sesi√≥n
if processed_content:
    st.subheader(f"Archivo en Sesi√≥n: {file_name_to_display}")
    
    # Mostrar preview
    if isinstance(processed_content, Image.Image):
        st.image(processed_content, caption="Imagen cargada", width=300)
    else:
        st.text_area("Previsualizaci√≥n:", value=processed_content, height=100)
    
    # Bot√≥n para limpiar la subida de sesi√≥n
    if st.button("üóëÔ∏è Eliminar archivo de la sesi√≥n"):
        st.session_state.uploaded_file_data = None
        st.session_state.uploaded_file_name = None
        # Forzar la recarga para que el file_uploader se resetee visualmente
        st.rerun() 

# =================================================================
# BLOQUE DE CARGA DE ARCHIVOS (MOVIMIENTO HACIA ARRIBA)
# =================================================================
with st.expander("üìÇ Cargar Archivos (Im√°genes o Texto)", expanded=True):
    # Usar un widget file_uploader para permitir la selecci√≥n de archivos.
    # El archivo subido aqu√≠ espera una acci√≥n para ser guardado en la sesi√≥n.
    current_uploaded_file = st.file_uploader(
        "Arrastra tu archivo aqu√≠ (Pulsa 'Guardar' para enviarlo a la sesi√≥n de chat)", 
        type=["jpg", "png", "txt", "csv", "py", "md"], 
        # Clave √∫nica para evitar errores de widget si el estado cambia
        key="file_uploader_widget"
    )
    
    # L√≥gica de bot√≥n: Solo guardar si hay un archivo seleccionado y se pulsa el bot√≥n.
    if current_uploaded_file is not None:
        if st.button("üíæ Guardar Archivo en Sesi√≥n", key="save_file_btn"):
            # L√≥gica de persistencia: Guarda el archivo subido
            
            # Chequea si es un nuevo archivo (o si queremos re-procesar el mismo)
            if st.session_state.uploaded_file_name != current_uploaded_file.name:
                st.session_state.uploaded_file_name = current_uploaded_file.name
            
            # Es crucial volver al inicio del buffer antes de leer
            current_uploaded_file.seek(0)
            
            # Guardamos el archivo procesado en la sesi√≥n
            st.session_state.uploaded_file_data = process_uploaded_file(current_uploaded_file)
            st.toast(f"Archivo '{current_uploaded_file.name}' cargado a la sesi√≥n. ¬°Listo para chatear!", icon='üíæ')
            # Forzamos un rerun para que el preview superior se actualice inmediatamente.
            st.rerun()
            

# =================================================================
# FIN DEL BLOQUE DE CARGA DE ARCHIVOS
# =================================================================

# --- BOTONES DE ACCIONES R√ÅPIDAS (NUEVA SECCI√ìN) ---
st.markdown("---")
st.subheader("üöÄ Acciones R√°pidas")
col1, col2 = st.columns(2)

# Bot√≥n 1: Asesor√≠a R√°pida (Iniciar Proyecto)
if col1.button("üß† Asesor√≠a R√°pida (Iniciar Proyecto)", use_container_width=True):
    # Almacena el prompt para ser procesado inmediatamente despu√©s del rerun
    st.session_state.prompt_from_button = "Desearia ver ejemplos"
    st.rerun() # Fuerza la re-ejecuci√≥n del script para entrar en la l√≥gica del chat

# Bot√≥n 2: Descargar Formatos Esenciales
format_content = """
# Plantillas y Formatos de Proyecto

Aqu√≠ tienes enlaces a formatos esenciales que podr√≠as necesitar:

Tipo de documento (instructivo, gu√≠a o procedimiento)
T√≠tulo del documento
Objetivo
Alcance
Contexto o proceso asociado
Actividades o pasos
Responsables
Registros o evidencias
Indicaciones de formato institucional

---
*Nota: En una aplicaci√≥n real, estos ser√≠an enlaces directos de descarga.*
"""
col2.download_button(
    label="‚¨áÔ∏è Descargar Formatos Esenciales",
    data=format_content,
    file_name="Formatos_Esenciales_AliadoDoc.md",
    mime="text/markdown",
    use_container_width=True
)
st.markdown("---")

# Inicializar historial de chat en session_state si no existe
if "messages" not in st.session_state:
    # Mensaje inicial del asistente (la "gem" de bienvenida)
    st.session_state.messages = [{
        "role": "assistant",
        "content": "¬°Hola! Soy AliadoDoc. Puedes subir una imagen o archivo de texto para que lo analice, o usar los botones de 'Acciones R√°pidas' para comenzar."
    }]

# Mostrar mensajes anteriores
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])


# --- L√≥gica de Chat Unificada ---
# 1. Obtener prompt del bot√≥n (si existe) y limpiarlo del estado de sesi√≥n.
prompt_from_button = st.session_state.pop('prompt_from_button', None)
# 2. Obtener prompt del input de chat.
prompt_from_chat = st.chat_input("Escribe tu mensaje...")

# Determinar el prompt final a usar
user_prompt = prompt_from_button or prompt_from_chat

if user_prompt:
    # Agregar el mensaje del usuario al historial
    st.session_state.messages.append({"role": "user", "content": user_prompt})
    with st.chat_message("user"):
        st.markdown(user_prompt)

    if not api_key:
        st.warning("‚ö†Ô∏è Necesitas una API Key. Por favor, configura el Secreto de Colab llamado GEMINI_API_KEY.")
    else:
        with st.chat_message("assistant"):
            msg_placeholder = st.empty()
            full_response = ""
            
            # Si hay contenido en la sesi√≥n, se adjunta a la llamada a la API
            content_list = []
            if processed_content: content_list.append(processed_content)
            
            # Llamada a la API, pasando la instrucci√≥n del sistema y el prompt
            response_stream = get_gemini_response(api_key, model_option, user_prompt, SISTEMA_DE_CONDUCTA, content_list)
            
            if isinstance(response_stream, str):
                msg_placeholder.markdown(response_stream)
                full_response = response_stream
            else:
                try:
                    for chunk in response_stream:
                        if chunk.text:
                            full_response += chunk.text
                            # Muestra la respuesta en tiempo real
                            msg_placeholder.markdown(full_response + "‚ñå")
                    msg_placeholder.markdown(full_response)
                except Exception as e:
                    st.error(f"Error: {e}")
            
            st.session_state.messages.append({"role": "assistant", "content": full_response})